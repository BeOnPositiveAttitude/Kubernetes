Предположим у нас есть один сервер с ETCD. Но это база данных и она может хранить важные данные. Поэтому возможно разместить хранилище данных на нескольких серверах.

Теперь у нас есть три сервера, на всех запущен ETCD, и все они обслуживают одинаковую копию БД. Соответственно, если вы потеряете один сервер, то все еще будете иметь две копии данных. Но каким образом ETCD обеспечивает согласованность (консистентность) данных на всех нодах?

Вы можете осуществлять запись на любой экземпляр и читать данные с любого экземпляра. ETCD гарантирует, что одинаковая согласованная копия данных доступна одновременно на всех экземплярах.

Как он это делает? С чтением всем достаточно просто. Т.к. одинаковые данные доступны на всех нодах, то вы можете легко прочитать их с любой ноды. Но не в случае с записью. Что если два запроса на запись придут на два разных экземпляра? Какой из них пройдет?

Например, на одну ноду пришел запрос на запись для имени `John`, а на другую ноду для имени `Joe`. Конечно мы не можем иметь разные данные на двух разных нодах.

<img src="image-3.png" width="600" height="300"><br>

Когда мы сказали, что ETCD может осуществлять запись через любой экземпляр, то не были правы на 100%. ETCD не обрабатывает запросы на запись на каждой ноде. Вместо этого только один из экземпляров отвечает за обработку запросов на запись.

Внутри две ноды выбирают среди себя лидера. Из общего числа экземпляров одна нода становится лидером, а другие ноды становятся последователями (followers).

Если запрос на запись пришел через лидер-ноду, тогда лидер и обработает этот запрос. Затем лидер убеждается, что другим нодам отправлена копия данных.

Если запрос на запись пришел через любую другую follower-ноду, тогда она перенаправит запрос на запись лидеру, а лидер обработает этот запрос.

И снова, когда запрос на запись обработан, лидер убеждается, что копии записи распространены другим экземплярам кластера.

Таким образом запись считается завершенной, если лидер получил согласие от других членов кластера.

Каким образом ноды выбирают среди себя лидера? И как они обеспечивают распространение записи по всем экземплярам? ETCD реализует распределенный консенсус (согласие, единодушие), используя протокол RAFT. Посмотрим как это работает в кластере из трех нод.

Когда кластер установлен, мы имеем три ноды, у которых не выбран лидер. Алгоритм RAFT использует рандомные таймеры для инициации запросов. Например, рандомный таймер запускается у трех менеджеров. Первый, у кого закончится таймер, посылает запрос другим нодам, запрашивающий разрешение стать лидером. Другие менеджеры, получив запрос, отвечают своим голосом (голосуют) и нода принимает на себя роль лидера. Теперь, когда нода избрана лидером, она посылает нотификации через регулярный промежуток времени другим мастерам, информируя их о том, что она продолжает брать на себя роль лидера.

В случае, если другие ноды в какой-то момент времени не получают уведомлений от лидера, что может произойти либо из-за выхода лидера из строя либо потери сетевой связности, ноды инициируют среди себя процесс переизбрания и определяется новый лидер.

Возвращаясь к предыдущем примеру, когда приходит запрос на запись, он обрабатывается лидером и реплицируется на другие ноды кластера. Запись считается завершенной только, когда она скопирована (распространена) на другие экземпляры кластера.

Мы сказали, что ETCD является высокодоступным. Даже если мы теряем ноду, он все еще должен функционировать.

Например пришел новый запрос на запись, но одна из нод кластера не отвечает. Следовательно лидер имеет возможность записи только на две ноды кластера. Будет ли запись считаться завершенной в этом случае? Будет ли система ждать, когда третья нода вернется в строй? Или запись провалится?

**Запись считается завершенной в том случае, если она может быть выполнена на большинстве (majority) нод кластера**. Например в случае трех нод, значение majority равно 2. Соответственно, если данные могут быть записаны на две ноды из трех, тогда запись будет считаться завершенной.

Если третья нода вернется онлайн, то данные затем также будут скопированы и на нее.

Что такое majority? Более подходящим для использования термином будет *кворум*. Кворум - это минимальное количество нод, которые должны быть доступны для правильной работы кластера или для успешного выполнения записи. В случае трех нод, значение равно двум. Для любого заданного количества нод кворум вычисляется по формуле:

`Quorum = N/2 + 1`

Для трех нод: `Quorum = 3/2 + 1 = 2.5 ~= 2`. Если есть значение после запятой `.5`, то считается только целое число, поэтому 2.

Для пяти нод: `Quorum = 5/2 + 1 = 3.5 ~= 3`.

Ниже представлена таблица, показывающая кворум кластеров размером от 1 до 7.

<img src="image-4.png" width="150" height="300"><br>

В случае single node кластера ничего из этого на самом деле не применимо. Если вы потеряете эту ноду, то потеряете все.

Для двух нод: `Quorum = 2/2 + 1 = 2`. Соответственно, даже если у вас два экземпляра в кластера, то majority все еще равно 2. Если одна нода выйдет из строя, то не будет кворума, а запись не сможет быть обработана. Поэтому иметь два экземпляра — это все равно, что иметь один экземпляр. Он не представляет какой-либо реальной ценности, поскольку кворум не может быть достигнут.

Поэтому рекомендуется иметь минимум три экземпляра в ETCD кластере. В этом случае он обеспечивает отказоустойчивость (fault tolerance) как минимум одного узла. Если вы потеряете одну ноду, то все еще сможете иметь кворум и кластер будет продолжать функционировать.

Первая колонка таблицы минус вторая дает нам значение отказоустойчивости - количество нод, которое мы можем позволить себе потерять, сохраняя при этом кластер работоспособным.

<img src="image-6.png" width="250" height="300"><br>

Кластеры из одной и двух нод вне рассмотрения. В случае от 3 до 7 нод, какие варианты стоит рассматривать?

3 и 4 имеют одинаково значение fault tolerance, равное 1.

5 и 6 также имеют одинаково значение fault tolerance, равное 2.

При принятии решения о количестве master-нод рекомендуется выбирать **нечетное** число - 3, 5 или 7.

Предположим у нас есть кластер из 6 нод. Из-за сбоя сеть выходит из строя, что приводит к ее разделению. Мы имеем 4 ноды в одном сегменте и 2 в другом. В этом случае группа из четырех нод имеет кворум и продолжает нормально работать. Однако в случае, если сеть разделилась другим образом, приведя к тому, что ноды равномерно распределились между двумя сегментами, каждая группа теперь имеет только 3 ноды. 