Daemon Set также как и ReplicaSet позволяет разворачивать несколько экземпляров pod-а. Но он запускает **одну** копию pod-а на каждой ноде кластера. Всякий раз когда новая нода добавляется в кластер, реплика pod-а автоматически добавляется на эту ноду. Соответственно когда нода удаляется, автоматически удаляется и pod.

Daemon Set гарантирует, что одна копия pod-а всегда будет присутствовать на всех нодах кластера.

В каких случаях используется Daemon Set? Например, если вы хотите развернут агент мониторинга или лог-коллектор на каждой ноде кластера. В этом случае Daemon Set отлично подойдет, он может развернуть агент мониторинга в виде pod-а на всех нодах кластера.

Еще один из хороших примеров использования Daemon Set - это компонент кластера kube-proxy.

Другой случай использования Daemon Set - различные networking solutions, например *weave-net* требует, чтобы агент был развернут на каждой ноде кластера.

Создание Daemon Set похоже на процесс создания Replica Set.

```yaml
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: monitoring-daemon
spec:
  selector:
    matchLabels:
      app: monitoring-agent
  template:
    metadata:
      labels:
        app: monitoring-agent
    spec:
      containers:
        - name: monitoring-agent
          image: monitoring-agent
```

Создать Daemon Set: `kubectl create -f daemon-set-definition.yaml`.

Смотреть созданные Daemon Sets: `kubectl get daemonsets` или более кратко `kubectl get ds`.

Как работает Daemon Set? Как он планирует pod-ы на каждую ноду? И каким образом он гарантирует, что на каждой ноде будет pod?

Предположим вам попросили запланировать pod на каждую ноду кластера. Как бы вы это сделали? Как уже знаем, можно задать свойство `nodeName:` в манифест-файле pod-а, чтобы обойти scheduler и разместить pod напрямую на ноду. Это первый подход - в каждом pod-е установить свойство `nodeName:` перед его созданием и когда они создадутся, то автоматически попадут на соответствующие ноды. Так было до версии K8s v1.12. Со следующих версий Daemon Set стал использовать дефолтный scheduler и node affinity rules.