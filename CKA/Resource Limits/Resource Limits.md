K8s Scheduler решает на какую ноду назначить pod.

Scheduler смотрит сколько ресурсов требует pod и сколько доступно на ноде и в зависимости от этого принимает решение на какую ноду поместить pod.

Если ни на одной из нод кластера не будет достаточно свободных ресурсов, требуемых для размещения pod-а, тогда Scheduler приостановит размещение pod-а и он будет висеть в статусе Pending. Если при этом посмотреть события с помощью команды `kubectl describe pod nginx`, то можно увидеть сообщение "Insufficient cpu".

*Resource request* - минимальное количество cpu или ram, необходимое для запуска для контейнера.

Когда Scheduler пытается определить ноду с достаточным количеством ресурсов для размещения pod-а, он ориентируется на эти значения.

Мы можем определить это в definition файле pod-а или Deployment в блоке `resources` => `requests`:

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: simple-webapp-color
  labels:
    name: simple-webapp-color
spec:
  containers:
    - name: simple-webapp-color
      image: simple-webapp-color
      ports:
        - containerPort: 8080
      resources:
        requests:
          memory: "4Gi"
          cpu: 2
```

Для cpu можно указать значение например 0.1 или 100m (милли), минимально 1m.

1 CPU = 1 AWS vCPU = 1 GCP Core = 1 Azure Core = 1 Hyperthread

<img src="screenshot.png" width="300" height="200"><br>

По умолчанию контейнер не имеет ограничений на потребление ресурсов ноды, соответственно может скушать вообще все ресурсы ноды и "задушить" процессы самого хоста.

Мы можем задать лимиты на использование ресурсов в definition файле pod-а или Deployment в блоке `resources` => `limits`:

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: simple-webapp-color
  labels:
    name: simple-webapp-color
spec:
  containers:
    - name: simple-webapp-color
      image: simple-webapp-color
      ports:
        - containerPort: 8080
      resources:
        requests:
          memory: "1Gi"
          cpu: 1
        limits:
          memory: "2Gi"
          cpu: 2
```

Важно! limits и requests устанавливаются для каждого контейнера в pod-е.

Если контейнер в pod-е попытается выйти за рамки лимита по CPU, система ограничит (throttle) его потребление заданными рамками. Т.е. контейнер не сможет использовать больше CPU, чем задано в `limits`.

Что касается RAM, то контейнер может выйти за установленный лимит по памяти, но в таком случае pod будет остановлен (terminated) с ошибкой OOM (Out of Memory), которую можно будет увидеть в выводе команды `kubectl describe pod nginx`.

По умолчанию в K8s не заданы какие-либо дефолтные значения для `requests` и `limits`. Это значит, что любой pod может использовать столько ресурсов, сколько ему нужно на любой ноде и "задушить" другие pod-ы или процессы на этой ноде. Это очень-очень важно знать!

Рассмотрим как работают `requests` и `limits` для CPU.

Предположим существует два pod-а, конкурирующие за ресурсы кластера. В данном случае, когда мы говорим pod, то имеем ввиду контейнер внутри pod-а.

Без установленных `requests` и `limits`, один pod может "употребить" все ресурсы CPU на ноде и помешать другому pod-у получить необходимые ресурсы.

Другой вариант - заданы `limits`, но не заданы `requests`. В этом случае K8s автоматически задаст для `requests` такое же значение, что установлено для `limits`. Например для `requests` и `limits` принято значение 3, тогда для каждого pod-а будет гарантированно выделено 3 vCPU и не больше.

Следующий вариант - заданы и `requests` и `limits`. В этом случае каждый pod получит гарантированное количество запрошенных CPU - 1vCPU и сможет вырасти до 3vCPU, но не более. Этот вариант выглядит более подходящим, но есть один недостаток. Если первый pod по некоторым причинам потребует большее количество тактов CPU, а второй pod при этом НЕ будет реально потреблять много CPU, то в этом случае мы не захотим ограничивать потребление первого pod-а в плане CPU. Мы хотим разрешить первому pod-у потреблять доступное количество тактов CPU, пока второй pod в них действительно не нуждается.

И последний сценарий - заданы `requests`, но не заданы `limits`. Т.к. в данном случае заданы `requests`, то каждый pod гарантированно получит 1 vCPU. Однако т.к. не заданы `limits`, любой pod в случае необходимости сможет "употребить" столько vCPU сколько доступно. Но в любой момент времени, если второму pod-у потребуются дополнительные такты CPU

<img src="image.png" width="900" height="300"><br>

Задание небольшого CPU requests даёт pod-у хорошие шансы быть запланированным

Установка limits на ресурсы CPU, большего, чем requests, позволяет достичь 2 вещей:
- при увеличении нагрузки pod может задействовать дополнительные ресурсы CPU
- количество ресурсов CPU, которые pod может задействовать при повышении нагрузки, ограничено некоторой разумной величиной
